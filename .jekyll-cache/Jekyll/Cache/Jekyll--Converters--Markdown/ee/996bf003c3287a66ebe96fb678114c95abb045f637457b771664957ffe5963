I"­	<h2>The Problem</h2>

<p>In recent years, along with the rise of machine learning across a wide plethora of applications, we see massive troves of data being collected from individuals. Be it usersâ€™ online activity or diagnosis reports, large amounts of sensitive data are collected by a growing number of institutions, ranging from hospitals and government bodies to private companies and research organizations. This has ushered growing concerns associated with a potential loss of user privacy.</p>

<h2>What I did</h2>

<p>With growing concerns over privacy, this work aims to study privacy preservation
techniques using the differential privacy framework and, draw comparisons
between medical and non-medical datasets from a privacy standpoint. We compare
the use of Differentially Private Stochastic Gradient Descent(DP-SGD) and
Private Aggregation of Teacher Ensembles(PATE) on three publicly available
datasets.</p>

<p><img class="center" src="/img/portfolio/DP_diagram.png" alt="" title="differential-privacy diagram" /></p>
<div class="col three caption">
	Figure 1: Comparison between Global and Local Differential Privacy
</div>

<h2> Take Away
In conclusion, the importance of this research stems from the need to preserve privacy when training deep learning models with sensitive data. With the rise of the use of Deep Learning in the healthcare domain, it is crucial to quantify and analyze privacy. In the past decade, Differential Privacy has become the de-facto framework for performing privacy analysis. There are numerous relaxations of Differential Privacy with the most prominent ones being ($\epsilon$,$\delta$)-Differential Privacy(DP) and R\'enyi Differential Privacy(RDP). We have used RDP for calculations and expressed results in ($\epsilon$,$\delta$)-DP for easier interpretation. This work particularly attempted to evaluate how healthcare datasets performed compared to a control dataset. Healthcare datasets are usually plagued with missing values and class imbalance problems. It was found that these factors affected the utility privacy trade-off significantly. The most important factors were found to be the complexity of the dataset and the type of noise applied. Future work will hence aim to test varying complexities of datasets and type of noise applied. The hope of this work is to be able to build on the foundation of existing work on Differential Privacy and extend them to particular cases within the healthcare industry.
</h2>
:ET